{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03f7a65",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# --- Numerical & Data Science ---\n",
    "import numpy as np\n",
    "\n",
    "# --- Machine Learning & Metrics ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# --- Audio Processing ---\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# --- Video Processing ---\n",
    "import cv2\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "# --- TensorFlow (for YAMNet) ---\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "# --- Torchaudio (for VGGish) ---\n",
    "import torchaudio\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Warnings ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746b5a4",
   "metadata": {},
   "source": [
    "## Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ff5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_videos(base_path, event_types, train_ratio=0.8, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split videos into train and test subfolders for each event type based on unequal sample sizes.\n",
    "    For \"Red card\" and \"Penalty\", ensure all three samples of an event (normal, before, after) remain together.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the base folder containing event folders.\n",
    "        event_types (list): List of event types to process.\n",
    "        train_ratio (float): Proportion of videos to place in the train folder.\n",
    "        test_ratio (float): Proportion of videos to place in the test folder.\n",
    "    \"\"\"\n",
    "    for event in event_types:\n",
    "        event_folder = os.path.join(base_path, event.replace(\" \", \"_\"))\n",
    "        train_folder = os.path.join(event_folder, \"train\")\n",
    "        test_folder = os.path.join(event_folder, \"test\")\n",
    "        \n",
    "        # Create train and test subfolders if they don't exist\n",
    "        os.makedirs(train_folder, exist_ok=True)\n",
    "        os.makedirs(test_folder, exist_ok=True)\n",
    "        \n",
    "        # Get all video files in the event folder\n",
    "        video_files = [f for f in os.listdir(event_folder) if f.endswith(\".mp4\")]\n",
    "        \n",
    "        if event in [\"Red card\", \"Penalty\"]:\n",
    "            # Group files by event (normal, before, after)\n",
    "            grouped_files = defaultdict(list)\n",
    "            for filename in video_files:\n",
    "                # Extract the base event identifier (e.g., \"1_match_normal\", \"1_match_before\", \"1_match_after\")\n",
    "                base_event = \"_\".join(filename.split(\"_\")[:2])  # First two parts of the filename\n",
    "                grouped_files[base_event].append(filename)\n",
    "            \n",
    "            # Shuffle the grouped events to ensure randomness\n",
    "            grouped_events = list(grouped_files.items())\n",
    "            random.shuffle(grouped_events)\n",
    "            \n",
    "            # Calculate the number of train and test events\n",
    "            total_events = len(grouped_events)\n",
    "            num_train_events = int(total_events * train_ratio)\n",
    "            num_test_events = total_events - num_train_events\n",
    "            \n",
    "            # Split events into train and test sets\n",
    "            train_events = grouped_events[:num_train_events]\n",
    "            test_events = grouped_events[num_train_events:]\n",
    "            \n",
    "            # Move grouped files to train folder\n",
    "            for base_event, files in train_events:\n",
    "                for file in files:\n",
    "                    src_path = os.path.join(event_folder, file)\n",
    "                    dest_path = os.path.join(train_folder, file)\n",
    "                    shutil.move(src_path, dest_path)\n",
    "            \n",
    "            # Move grouped files to test folder\n",
    "            for base_event, files in test_events:\n",
    "                for file in files:\n",
    "                    src_path = os.path.join(event_folder, file)\n",
    "                    dest_path = os.path.join(test_folder, file)\n",
    "                    shutil.move(src_path, dest_path)\n",
    "            \n",
    "            print(f\"Event: {event}\")\n",
    "            print(f\"  Total events: {total_events}\")\n",
    "            print(f\"  Train events: {len(train_events)}\")\n",
    "            print(f\"  Test events: {len(test_events)}\")\n",
    "        else:\n",
    "            # Shuffle the video files to ensure randomness\n",
    "            random.shuffle(video_files)\n",
    "            \n",
    "            # Calculate the number of train and test videos\n",
    "            total_videos = len(video_files)\n",
    "            num_train_videos = int(total_videos * train_ratio)\n",
    "            num_test_videos = total_videos - num_train_videos\n",
    "            \n",
    "            # Split videos into train and test sets\n",
    "            train_videos = video_files[:num_train_videos]\n",
    "            test_videos = video_files[num_train_videos:]\n",
    "            \n",
    "            # Move videos to train folder\n",
    "            for video in train_videos:\n",
    "                src_path = os.path.join(event_folder, video)\n",
    "                dest_path = os.path.join(train_folder, video)\n",
    "                shutil.move(src_path, dest_path)\n",
    "            \n",
    "            # Move videos to test folder\n",
    "            for video in test_videos:\n",
    "                src_path = os.path.join(event_folder, video)\n",
    "                dest_path = os.path.join(test_folder, video)\n",
    "                shutil.move(src_path, dest_path)\n",
    "            \n",
    "            print(f\"Event: {event}\")\n",
    "            print(f\"  Total videos: {total_videos}\")\n",
    "            print(f\"  Train videos: {len(train_videos)}\")\n",
    "            print(f\"  Test videos: {len(test_videos)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define base paths for Clips and Features-processed\n",
    "    base_paths = [\n",
    "        \"F:/AIM Lab/Experiment/sliding-window/Clips\"\n",
    "    ]\n",
    "    \n",
    "    # List of event types to process\n",
    "    event_types = [\"Goal\", \"Red_card\", \"Yellow_card\", \"Direct_free-kick\", \"Penalty\", \"Indirect_free-kick\", \"Corner\", \"Substitution\", \"Shots_on_target\", \"no_event\"]\n",
    "    \n",
    "    # Split videos in both base paths\n",
    "    for base_path in base_paths:\n",
    "        split_videos(base_path, event_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489b59b",
   "metadata": {},
   "source": [
    "## Handy Crafted Features from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract compact handcrafted video features per clip:\n",
    " - Optical Flow statistics (6-8 dims)\n",
    " - Shot-transition / cut-rate features (3-4 dims)\n",
    "\"\"\"\n",
    "\n",
    "VIDEO_EXTS = {\".mp4\", \".mkv\"}\n",
    "\n",
    "\n",
    "def is_video_file(path):\n",
    "    return os.path.splitext(path)[1].lower() in VIDEO_EXTS\n",
    "\n",
    "\n",
    "def read_video_frames(video_path, max_frames=None, resize_shorter=640):\n",
    "    \"\"\"\n",
    "    Read all frames from video (grayscale), optionally resizing for speed.\n",
    "    Returns list of frames (uint8) in grayscale.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        # convert to grayscale for optical flow & difference\n",
    "        if resize_shorter is not None:\n",
    "            h, w = frame.shape[:2]\n",
    "            scale = resize_shorter / float(min(h, w))\n",
    "            if scale < 1.0:\n",
    "                frame = cv2.resize(frame, (int(w * scale), int(h * scale)),\n",
    "                                   interpolation=cv2.INTER_AREA)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(gray)\n",
    "        success, frame = cap.read()\n",
    "        if max_frames is not None and len(frames) >= max_frames:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames, fps\n",
    "\n",
    "\n",
    "\n",
    "def optical_flow_stats(frames, flow_method=\"farneback\", smooth_window=3):\n",
    "    \"\"\"\n",
    "    Compute per-frame optical-flow magnitude statistics and summarize.\n",
    "    \"\"\"\n",
    "    T = len(frames)\n",
    "    if T < 2:\n",
    "        return np.zeros(8, dtype=np.float32)\n",
    "\n",
    "    mags = []\n",
    "    prev = frames[0]\n",
    "\n",
    "    # Try fast DIS optical flow if available (opencv-contrib), else lighter Farneback\n",
    "    use_dis = (flow_method == \"dis\")\n",
    "    if use_dis and hasattr(cv2, \"DISOpticalFlow_create\"):\n",
    "        dis = cv2.DISOpticalFlow_create(cv2.DISOPTICAL_FLOW_PRESET_FAST)\n",
    "\n",
    "    for i in range(1, T):\n",
    "        curr = frames[i]\n",
    "        if use_dis and hasattr(cv2, \"DISOpticalFlow_create\"):\n",
    "            flow = dis.calc(prev, curr, None)\n",
    "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        else:\n",
    "            # Relaxed Farneback params for speed\n",
    "            flow = cv2.calcOpticalFlowFarneback(\n",
    "                prev, curr, None,\n",
    "                pyr_scale=0.5, levels=2, winsize=9,\n",
    "                iterations=2, poly_n=5, poly_sigma=1.1, flags=0\n",
    "            )\n",
    "            mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        m = float(np.mean(mag))\n",
    "        mags.append(m)\n",
    "        prev = curr\n",
    "\n",
    "    mags = np.array(mags)\n",
    "    if smooth_window > 1 and len(mags) >= smooth_window:\n",
    "        kernel = np.ones(smooth_window) / smooth_window\n",
    "        mags = np.convolve(mags, kernel, mode=\"same\")\n",
    "\n",
    "    mean_m = mags.mean()\n",
    "    std_m = mags.std()\n",
    "    max_m = mags.max()\n",
    "    median_m = np.median(mags)\n",
    "    q25 = np.percentile(mags, 25)\n",
    "    q75 = np.percentile(mags, 75)\n",
    "    idx_max = int(np.argmax(mags))\n",
    "    time_of_max = idx_max / max(1, (len(mags) - 1))\n",
    "    k = 1.0\n",
    "    threshold = mean_m + k * std_m\n",
    "    peaks = int(np.sum((mags[1:-1] > mags[:-2]) & (mags[1:-1] > mags[2:]) & (mags[1:-1] > threshold)))\n",
    "\n",
    "    vec= np.array([mean_m, std_m, max_m, median_m, q25, q75, time_of_max, peaks], dtype=np.float32)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def shot_transition_features(frames, fps, downscale=0.25, cut_k=2.0):\n",
    "    \"\"\"\n",
    "    Detect shot cuts via frame-to-frame difference on downscaled grayscale frames.\n",
    "    Returns vector of shot features: num_cuts, cuts_per_sec, median_shot_len_sec, shot_len_std\n",
    "    \"\"\"\n",
    "    T = len(frames)\n",
    "    if T < 2:\n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "\n",
    "    # downsample frames for diff speed\n",
    "    small_frames = []\n",
    "    for f in frames:\n",
    "        h, w = f.shape[:2]\n",
    "        nw, nh = max(1, int(w * downscale)), max(1, int(h * downscale))\n",
    "        small_frames.append(cv2.resize(f, (nw, nh), interpolation=cv2.INTER_AREA).astype(np.float32) / 255.0)\n",
    "\n",
    "    diffs = []\n",
    "    for i in range(1, len(small_frames)):\n",
    "        # L1/frame mean\n",
    "        d = np.mean(np.abs(small_frames[i] - small_frames[i - 1]))\n",
    "        diffs.append(d)\n",
    "    diffs = np.array(diffs)\n",
    "\n",
    "    # threshold adaptively: mean + cut_k * std\n",
    "    mu, sigma = diffs.mean(), diffs.std()\n",
    "    thr = mu + cut_k * sigma\n",
    "    cut_flags = diffs > thr\n",
    "    num_cuts = int(cut_flags.sum())\n",
    "\n",
    "    # shot lengths in frames: segments between cuts\n",
    "    # if no cuts: one shot of full length\n",
    "    cut_indices = np.where(cut_flags)[0].tolist()\n",
    "    # convert diffs idx -> cut occurs between frame i and i+1; shot boundaries:\n",
    "    boundaries = [0] + [i + 1 for i in cut_indices] + [T]\n",
    "    shot_lengths = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        shot_len = boundaries[i + 1] - boundaries[i]\n",
    "        shot_lengths.append(shot_len)\n",
    "    shot_lengths = np.array(shot_lengths, dtype=np.float32)\n",
    "    shot_len_secs = shot_lengths / max(1.0, fps)\n",
    "\n",
    "    median_shot_len = float(np.median(shot_len_secs))\n",
    "    shot_len_std = float(np.std(shot_len_secs))\n",
    "    clip_duration_sec = max(1.0, T / max(1.0, fps))\n",
    "    cuts_per_sec = num_cuts / clip_duration_sec\n",
    "\n",
    "    vec = np.array([num_cuts, cuts_per_sec, median_shot_len, shot_len_std], dtype=np.float32)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def process_clip(video_path):\n",
    "    \"\"\"\n",
    "    Process a single video clip and return a compact handcrafted embedding.\n",
    "    Embedding layout (approx 12 dims):\n",
    "      [opt_flow:8] + [shot_feats:4] => total 12 dims (float32)\n",
    "    \"\"\"\n",
    "    frames, fps = read_video_frames(video_path, resize_shorter=640)\n",
    "    if len(frames) < 2:\n",
    "        # create zero embedding\n",
    "        embedding = np.zeros(12, dtype=np.float32)\n",
    "        return embedding\n",
    "\n",
    "    opt_vec = optical_flow_stats(frames, flow_method=\"dis\" if hasattr(cv2, \"DISOpticalFlow_create\") else \"farneback\")  # 8 dims\n",
    "    shot_vec = shot_transition_features(frames, fps)  # 4 dims\n",
    "\n",
    "    embedding = np.concatenate([opt_vec, shot_vec], axis=0)\n",
    "    return embedding\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "# add imports and a helper right after the above line\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import sys\n",
    "\n",
    "def _in_notebook():\n",
    "    # True when running under Jupyter/IPython kernel\n",
    "    return 'ipykernel' in sys.modules\n",
    "\n",
    "def _init_worker():\n",
    "    # Avoid oversubscription inside each worker\n",
    "    try:\n",
    "        cv2.setNumThreads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "\n",
    "def save_embedding(dest_base, class_name, split, clip_path, emb):\n",
    "    \"\"\"\n",
    "    Save embedding as numpy .npy with same relative structure under dest_base.\n",
    "    \"\"\"\n",
    "    stem = os.path.splitext(os.path.basename(clip_path))[0]\n",
    "    out_dir = os.path.join(dest_base, class_name, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, stem + \".npy\")\n",
    "    np.save(out_path, emb.astype(np.float32))\n",
    "    return out_path\n",
    "\n",
    "def _process_and_save(args):\n",
    "    cls, split, path, out_root = args\n",
    "    emb = process_clip(path)\n",
    "    save_embedding(out_root, cls, split, path, emb)\n",
    "    return path\n",
    "\n",
    "def main(clips_root=\"F:\\\\AIM Lab\\\\Experiment\\\\sliding-window\\\\Clips\", out_root=\"F:\\\\AIM Lab\\\\Experiment\\\\sliding-window\\\\video_features\", classes=None, max_workers=None):\n",
    "    if classes is None:\n",
    "        # try to auto-detect classes from folders\n",
    "        classes = [d for d in os.listdir(clips_root) if os.path.isdir(os.path.join(clips_root, d))]\n",
    "        classes.sort()\n",
    "\n",
    "    print(f\"Detected classes: {classes}\")\n",
    "    tasks = []\n",
    "    for cls in classes:\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            folder = os.path.join(clips_root, cls, split)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for fname in os.listdir(folder):\n",
    "                path = os.path.join(folder, fname)\n",
    "                if is_video_file(path):\n",
    "                    tasks.append((cls, split, path, out_root))\n",
    "\n",
    "    if not tasks:\n",
    "        print(\"No videos found.\")\n",
    "        return\n",
    "\n",
    "    workers = max_workers or max(1, (os.cpu_count() or 2) - 1)\n",
    "    use_threads = (_in_notebook() and os.name == 'nt')\n",
    "    print(f\"Processing {len(tasks)} files with {workers} {'threads' if use_threads else 'processes'}...\")\n",
    "    completed = 0\n",
    "    Executor = ThreadPoolExecutor if use_threads else ProcessPoolExecutor\n",
    "    init_kwargs = {} if use_threads else dict(initializer=_init_worker)\n",
    "\n",
    "    with Executor(max_workers=workers, **init_kwargs) as ex:\n",
    "        futures = [ex.submit(_process_and_save, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                _ = fut.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            completed += 1\n",
    "            if completed % 20 == 0:\n",
    "                print(f\"Done {completed}/{len(tasks)}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af570bd1",
   "metadata": {},
   "source": [
    "## Generating ResNet-50 embeddings for RGB video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet-50 model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Identity()  # Remove the final classifier to get 2048-d features\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Preprocessing pipeline for frames\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),                    # H×W×C → C×H×W, [0,1]\n",
    "    torchvision.transforms.Normalize(mean=[.485, .456, .406], std=[.229, .224, .225]),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "def embed_clip(frames: List[np.ndarray]):\n",
    "    \"\"\"\n",
    "    Generate ResNet-50 embeddings for a list of frames.\n",
    "    \n",
    "    Args:\n",
    "        frames (List[np.ndarray]): List of frames (H×W×C, BGR format).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Mean-pooled embedding (2048,) - single embedding per clip.\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for frame in frames:\n",
    "        # Convert BGR (OpenCV) to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        x = preprocess(frame_rgb).unsqueeze(0)  # 1×3×224×224\n",
    "        with torch.no_grad():\n",
    "            f = model(x)  # 1×2048\n",
    "        feats.append(f.squeeze(0).cpu().numpy())\n",
    "    \n",
    "    # Return mean-pooled embedding (single embedding per clip)\n",
    "    return np.mean(feats, axis=0)  # (2048,)\n",
    "\n",
    "def sample_frames_from_video(video_path, num_samples=16):\n",
    "    \"\"\"\n",
    "    Uniformly sample num_samples frames from the video at video_path.\n",
    "    Returns a list of BGR frames (as numpy arrays).\n",
    "    \"\"\"\n",
    "    print(f\"Opening video: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total frames in video: {total_frames}\")\n",
    "    if total_frames == 0:\n",
    "        print(f\"Error: No frames found in video {video_path}\")\n",
    "        return []\n",
    "    \n",
    "    dynamic_num_samples = num_samples\n",
    "    if total_frames > 250:\n",
    "        # Add 6 samples for every 100 frames after 200\n",
    "        additional_samples = ((total_frames - 200) // 100) * 6\n",
    "        dynamic_num_samples = num_samples + additional_samples\n",
    "        \n",
    "        print(f\"Video has {total_frames} frames (>250), using {dynamic_num_samples} samples \"\n",
    "              f\"(base {num_samples} + {additional_samples} additional)\")\n",
    "    else:\n",
    "        print(f\"Video has {total_frames} frames (≤250), using {dynamic_num_samples} samples\")\n",
    "    \n",
    "    # Sample frames\n",
    "    if total_frames < dynamic_num_samples:\n",
    "        # If fewer frames than samples, just read them all\n",
    "        indices = list(range(total_frames))\n",
    "        print(f\"Warning: Only {total_frames} frames available, using all of them\")\n",
    "    else:\n",
    "        # Uniformly spaced frame indices\n",
    "        indices = np.linspace(0, total_frames - 1, num=dynamic_num_samples, dtype=int)\n",
    "\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Warning: Failed to read frame {idx} from {video_path}\")\n",
    "            continue\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        print(f\"Error: No frames sampled from video {video_path}\")\n",
    "    else:\n",
    "        print(f\"Successfully sampled {len(frames)} frames from {video_path}\")\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def process_all_events(input_base_path, output_base_path, event_classes, num_samples=16):\n",
    "    \"\"\"\n",
    "    Process all video clips in train/test folders for each event class.\n",
    "    Generate ResNet-50 embeddings and save them maintaining the same directory structure.\n",
    "    \"\"\"\n",
    "    print(f\"Processing videos from: {input_base_path}\")\n",
    "    print(f\"Saving embeddings to: {output_base_path}\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    \n",
    "    for event in event_classes:\n",
    "        event_input_folder = os.path.join(input_base_path, event)\n",
    "        event_output_folder = os.path.join(output_base_path, event)\n",
    "        \n",
    "        if not os.path.exists(event_input_folder):\n",
    "            print(f\"Warning: Event folder not found: {event_input_folder}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing event: {event}\")\n",
    "        \n",
    "        # Process both train and test folders\n",
    "        for split in ['train', 'test']:\n",
    "            split_input_folder = os.path.join(event_input_folder, split)\n",
    "            split_output_folder = os.path.join(event_output_folder, split)\n",
    "            \n",
    "            if not os.path.exists(split_input_folder):\n",
    "                print(f\"  Warning: Split folder not found: {split_input_folder}\")\n",
    "                continue\n",
    "            \n",
    "            # Create output directory\n",
    "            os.makedirs(split_output_folder, exist_ok=True)\n",
    "            \n",
    "            print(f\"  Processing {event}/{split}...\")\n",
    "            \n",
    "            # Get all video files\n",
    "            video_files = [f for f in os.listdir(split_input_folder) if f.lower().endswith('.mp4')]\n",
    "            print(f\"    Found {len(video_files)} videos\")\n",
    "            \n",
    "            for fname in video_files:\n",
    "                video_path = os.path.join(split_input_folder, fname)\n",
    "                print(f\"    Processing video: {fname}\")\n",
    "                \n",
    "                # Sample frames from video\n",
    "                frames = sample_frames_from_video(video_path, num_samples=num_samples)\n",
    "                \n",
    "                if frames:\n",
    "                    # Generate mean-pooled embedding (single embedding per clip)\n",
    "                    embedding = embed_clip(frames)\n",
    "                    \n",
    "                    # Save the embedding as a .npy file with same name\n",
    "                    output_path = os.path.join(split_output_folder, f\"{os.path.splitext(fname)[0]}.npy\")\n",
    "                    np.save(output_path, embedding)\n",
    "                    print(f\"    Saved embedding to {output_path} (shape: {embedding.shape})\")\n",
    "                    total_processed += 1\n",
    "                else:\n",
    "                    print(f\"    Error: No frames sampled from {fname}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING COMPLETE!\")\n",
    "    print(f\"Total videos processed: {total_processed}\")\n",
    "    print(f\"Embeddings saved to: {output_base_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    input_base_path = \"F:/AIM Lab/Experiment/sliding-window/Clips\"\n",
    "    output_base_path = \"F:/AIM Lab/Experiment/sliding-window/Resnet-50 embeddings rgb\"\n",
    "    \n",
    "    # Define event classes\n",
    "    event_classes = [\n",
    "        \"Goal\", \"Red_card\", \"Yellow_card\", \"Direct_free-kick\", \"Penalty\", \n",
    "        \"Indirect_free-kick\", \"Corner\", \"Substitution\", \"Shots_on_target\", \"no_event\"\n",
    "    ]\n",
    "    \n",
    "    # Process all events with train/test structure\n",
    "    process_all_events(\n",
    "        input_base_path=input_base_path,\n",
    "        output_base_path=output_base_path,\n",
    "        event_classes=event_classes,\n",
    "        num_samples=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9a83f",
   "metadata": {},
   "source": [
    "## Audio Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_video(video_path, audio_path, duration=7.0):\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = video.audio\n",
    "        if audio is not None:\n",
    "            audio = audio.subclipped(0, min(duration, video.duration))\n",
    "            audio.write_audiofile(audio_path, fps=16000, codec='pcm_s16le', logger=None)\n",
    "            print(f\"Extracted audio: {audio_path}\")\n",
    "        else:\n",
    "            print(f\"No audio found in {video_path}\")\n",
    "        video.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio: {e}\")\n",
    "\n",
    "def batch_extract_audio(clips_dir, audio_dir):\n",
    "    os.makedirs(audio_dir, exist_ok=True)\n",
    "    for fname in os.listdir(clips_dir):\n",
    "        if fname.endswith('.mp4'):\n",
    "            video_path = os.path.join(clips_dir, fname)\n",
    "            audio_path = os.path.join(audio_dir, os.path.splitext(fname)[0] + '.wav')\n",
    "            extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "# Example usage for all events and splits\n",
    "base_clip_dir = \"F:/AIM Lab/Experiment/sliding-window/Clips\"\n",
    "base_audio_dir = \"F:/AIM Lab/Experiment/sliding-window/Audio\"\n",
    "event_classes =[\"Goal\", \"Red_card\", \"Yellow_card\", \"Direct_free-kick\", \"Penalty\", \"Indirect_free-kick\", \"Corner\", \"Substitution\", \"Shots_on_target\", \"no_event\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "for event in event_classes:\n",
    "    for split in splits:\n",
    "        clips_dir = os.path.join(base_clip_dir, event, split)\n",
    "        audio_dir = os.path.join(base_audio_dir, event, split)\n",
    "        batch_extract_audio(clips_dir, audio_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2e2d3",
   "metadata": {},
   "source": [
    "## Handy-Crafted features from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Your event classes\n",
    "event_classes = [\n",
    "    \"Goal\", \"Red_card\", \"Yellow_card\", \"Direct_free-kick\", \"Penalty\",\n",
    "    \"Indirect_free-kick\", \"Corner\", \"Substitution\", \"Shots_on_target\", \"no_event\"\n",
    "]\n",
    "\n",
    "# Directory containing the clips\n",
    "base_dir = \"F:\\\\AIM Lab\\\\Experiment\\\\sliding-window\\\\Audio\"\n",
    "\n",
    "# Base directory to save embeddings (same structure as clips/)\n",
    "out_base = \"F:\\\\AIM Lab\\\\Experiment\\\\sliding-window\\\\audio_features\"\n",
    "\n",
    "# MFCC config\n",
    "N_MFCC = 13\n",
    "\n",
    "def extract_audio_features(file_path, n_mfcc=N_MFCC):\n",
    "    \"\"\"\n",
    "    Extract aggregated MFCC stats (mean, std, max, min), average pitch, and avg RMS energy.\n",
    "    Returns:\n",
    "      features: 1D numpy array of length (n_mfcc*4 + 2)\n",
    "      feature_names: list of names in same order\n",
    "    \"\"\"\n",
    "    # load audio\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # MFCCs: shape (n_mfcc, time_frames)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # stats per MFCC coefficient across time\n",
    "    mfcc_mean = np.mean(mfccs, axis=1)\n",
    "    mfcc_std  = np.std(mfccs, axis=1)\n",
    "    mfcc_max  = np.max(mfccs, axis=1)\n",
    "    mfcc_min  = np.min(mfccs, axis=1)\n",
    "\n",
    "    # Average pitch (harmonic)\n",
    "    try:\n",
    "        y_harmonic, _ = librosa.effects.hpss(y)\n",
    "        pitches, magnitudes = librosa.piptrack(y=y_harmonic, sr=sr)\n",
    "        # Select pitch candidates where magnitude is relatively strong\n",
    "        mag_thresh = np.median(magnitudes[magnitudes > 0]) if np.any(magnitudes > 0) else 0.0\n",
    "        pitch_values = pitches[magnitudes > mag_thresh]\n",
    "        avg_pitch = float(np.mean(pitch_values)) if pitch_values.size > 0 else 0.0\n",
    "    except Exception:\n",
    "        avg_pitch = 0.0\n",
    "\n",
    "    # RMS energy\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    avg_rms = float(np.mean(rms)) if rms.size > 0 else 0.0\n",
    "\n",
    "    # concatenate in the order: mean, std, max, min, avg_pitch, avg_rms\n",
    "    features = np.hstack([mfcc_mean, mfcc_std, mfcc_max, mfcc_min, avg_pitch, avg_rms])\n",
    "\n",
    "    # build feature names for CSV header / debugging\n",
    "    names = []\n",
    "    for stat in [\"mean\", \"std\", \"max\", \"min\"]:\n",
    "        for i in range(n_mfcc):\n",
    "            names.append(f\"mfcc{i+1}_{stat}\")\n",
    "    names += [\"avg_pitch\", \"avg_rms\"]\n",
    "\n",
    "    return features.astype(np.float32), names\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Containers for CSV exports\n",
    "rows_train = []\n",
    "rows_test = []\n",
    "\n",
    "# Process files\n",
    "for split in [\"train\", \"test\"]:\n",
    "    for cls in event_classes:\n",
    "        in_folder = os.path.join(base_dir, cls, split)\n",
    "        out_folder = os.path.join(out_base, cls, split)\n",
    "        if not os.path.exists(in_folder):\n",
    "            # skip missing class/split folders silently\n",
    "            continue\n",
    "        ensure_dir(out_folder)\n",
    "\n",
    "        file_list = [f for f in os.listdir(in_folder) if f.lower().endswith(\".wav\")]\n",
    "        if not file_list:\n",
    "            continue\n",
    "\n",
    "        for fname in tqdm(file_list, desc=f\"{cls}/{split}\", leave=False):\n",
    "            in_path = os.path.join(in_folder, fname)\n",
    "            try:\n",
    "                feats, feat_names = extract_audio_features(in_path, n_mfcc=N_MFCC)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {in_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Save per-clip embedding .npy (same name as audio but .npy)\n",
    "            base_name = os.path.splitext(fname)[0]\n",
    "            out_path = os.path.join(out_folder, base_name + \".npy\")\n",
    "            np.save(out_path, feats)\n",
    "\n",
    "            # record metadata for CSV\n",
    "            row = {\"class\": cls, \"split\": split, \"audio_file\": in_path, \"embedding_file\": out_path}\n",
    "            # add features as columns (optional but handy)\n",
    "            for name, val in zip(feat_names, feats):\n",
    "                row[name] = float(val)\n",
    "            if split == \"train\":\n",
    "                rows_train.append(row)\n",
    "            else:\n",
    "                rows_test.append(row)\n",
    "\n",
    "# Create DataFrames and save aggregated CSVs\n",
    "if rows_train:\n",
    "    df_train = pd.DataFrame(rows_train)\n",
    "    df_train.to_csv(os.path.join(out_base, \"audio_features_train.csv\"), index=False)\n",
    "    print(\"Saved train CSV:\", os.path.join(out_base, \"audio_features_train.csv\"))\n",
    "else:\n",
    "    print(\"No train embeddings were generated.\")\n",
    "\n",
    "if rows_test:\n",
    "    df_test = pd.DataFrame(rows_test)\n",
    "    df_test.to_csv(os.path.join(out_base, \"audio_features_test.csv\"), index=False)\n",
    "    print(\"Saved test CSV:\", os.path.join(out_base, \"audio_features_test.csv\"))\n",
    "else:\n",
    "    print(\"No test embeddings were generated.\")\n",
    "\n",
    "# Print summary\n",
    "# Note: each embedding has length: N_MFCC * 4 + 2\n",
    "embed_len = N_MFCC * 4 + 2\n",
    "print(f\"Embedding dimension per clip: {embed_len}\")\n",
    "print(f\"Per-clip .npy embeddings saved under: {out_base}.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d672cb",
   "metadata": {},
   "source": [
    "## Generating YemNet Embeddings from audio (.wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)\n",
    "\n",
    "def yamnet_embed(logmel):\n",
    "    # YAMNet expects mono waveform, but we have logmel, so use the waveform directly\n",
    "    raise NotImplementedError(\"For TFHub YAMNet, pass waveform not logmel. Use .wav files directly.\")\n",
    "def batch_yamnet_embed(audio_dir, out_dir):\n",
    "    import soundfile as sf\n",
    "    import librosa\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for fname in os.listdir(audio_dir):\n",
    "        if fname.endswith('.wav'):\n",
    "            try:\n",
    "                # Load audio file\n",
    "                wav, sr = sf.read(os.path.join(audio_dir, fname))\n",
    "                \n",
    "                # Convert stereo to mono if needed\n",
    "                if len(wav.shape) == 2:\n",
    "                    wav = np.mean(wav, axis=1)\n",
    "                \n",
    "                # Resample to 16kHz if needed\n",
    "                if sr != 16000:\n",
    "                    wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "                \n",
    "                # ADAPTIVE PROCESSING FOR DIFFERENT DURATIONS\n",
    "                duration = len(wav) / 16000  # Duration in seconds\n",
    "                \n",
    "                if duration <= 7.0:\n",
    "                    # For short clips, pad to 7 seconds\n",
    "                    target_length = 16000 * 7\n",
    "                    if len(wav) < target_length:\n",
    "                        wav = np.pad(wav, (0, target_length - len(wav)))\n",
    "                    \n",
    "                    # Single YAMNet embedding\n",
    "                    scores, embeddings, spectrogram = yamnet_model(wav.astype(np.float32))\n",
    "                    emb = np.mean(embeddings.numpy(), axis=0)\n",
    "                    \n",
    "                elif duration <= 15.0:\n",
    "                    # For medium clips (7-15s), use sliding window approach\n",
    "                    window_size = 7 * 16000  # 7 seconds\n",
    "                    hop_size = 3 * 16000     # 3 seconds overlap\n",
    "                    \n",
    "                    embeddings_list = []\n",
    "                    start = 0\n",
    "                    while start + window_size <= len(wav):\n",
    "                        window = wav[start:start + window_size]\n",
    "                        scores, embeddings, spectrogram = yamnet_model(window.astype(np.float32))\n",
    "                        window_emb = np.mean(embeddings.numpy(), axis=0)\n",
    "                        embeddings_list.append(window_emb)\n",
    "                        start += hop_size\n",
    "                    \n",
    "                    # Average all window embeddings\n",
    "                    emb = np.mean(embeddings_list, axis=0)\n",
    "                    \n",
    "                else:\n",
    "                    # For long clips (>15s), sample multiple segments\n",
    "                    num_segments = min(5, int(duration // 3))  # Max 5 segments\n",
    "                    segment_length = 7 * 16000\n",
    "                    \n",
    "                    # Sample segments uniformly across the audio\n",
    "                    embeddings_list = []\n",
    "                    for i in range(num_segments):\n",
    "                        start_time = int((len(wav) - segment_length) * i / (num_segments - 1))\n",
    "                        segment = wav[start_time:start_time + segment_length]\n",
    "                        \n",
    "                        if len(segment) < segment_length:\n",
    "                            segment = np.pad(segment, (0, segment_length - len(segment)))\n",
    "                        \n",
    "                        scores, embeddings, spectrogram = yamnet_model(segment.astype(np.float32))\n",
    "                        segment_emb = np.mean(embeddings.numpy(), axis=0)\n",
    "                        embeddings_list.append(segment_emb)\n",
    "                    \n",
    "                    # Average all segment embeddings\n",
    "                    emb = np.mean(embeddings_list, axis=0)\n",
    "                \n",
    "                # Save embedding\n",
    "                output_path = os.path.join(out_dir, os.path.splitext(fname)[0] + '.npy')\n",
    "                np.save(output_path, emb)\n",
    "                print(f\"Saved audio embedding for {duration:.1f}s clip: {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {fname}: {e}\")\n",
    "\n",
    "# Example usage for all events and splits\n",
    "base_audio_dir = \"F:/AIM Lab/Experiment/sliding-window/Audio\"\n",
    "base_yamnet_dir = \"F:/AIM Lab/Experiment/sliding-window/Yamnet-embeddings\"\n",
    "event_classes =[\"Goal\", \"Red_card\", \"Yellow_card\", \"Direct_free-kick\", \"Penalty\", \"Indirect_free-kick\", \"Corner\", \"Substitution\", \"Shots_on_target\", \"no_event\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "for event in event_classes:\n",
    "    for split in splits:\n",
    "        audio_dir = os.path.join(base_audio_dir, event, split)\n",
    "        yamnet_dir = os.path.join(base_yamnet_dir, event, split)\n",
    "        batch_yamnet_embed(audio_dir, yamnet_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5dc6e",
   "metadata": {},
   "source": [
    "## Fusion of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def fuse_embeddings(resnet_emb, yamnet_emb, audio_features, video_features, fusion_method):\n",
    "    \n",
    "    if fusion_method == 'concat':\n",
    "        # Simple concatenation (3138-d)\n",
    "        return np.concatenate([resnet_emb,video_features, yamnet_emb, audio_features])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown fusion method: {fusion_method}\")\n",
    "\n",
    "def create_fused_embeddings(resnet_base_path, yamnet_base_path, audio_features_path, video_features_path, output_base_path, \n",
    "                          event_classes, fusion_method='concat'):\n",
    "    \n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Creating fused embeddings using method: {fusion_method}\")\n",
    "    print(f\"ResNet path: {resnet_base_path}\")\n",
    "    print(f\"YAMNet path: {yamnet_base_path}\")\n",
    "    print(f\"Audio features path: {audio_features_path}\")\n",
    "    print(f\"Video features path: {video_features_path}\")\n",
    "    print(f\"Output path: {output_base_path}\")\n",
    "    \n",
    "    total_fused = 0\n",
    "    \n",
    "    for event in event_classes:\n",
    "        for split in [\"Train\", \"Test\"]:\n",
    "            resnet_folder = os.path.join(resnet_base_path, event, split)\n",
    "            yamnet_folder = os.path.join(yamnet_base_path, event, split)\n",
    "            audio_features_folder = os.path.join(audio_features_path, event, split)\n",
    "            video_features_folder = os.path.join(video_features_path, event, split)\n",
    "            output_folder = os.path.join(output_base_path, event, split)\n",
    "            \n",
    "            # Create output directory\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(resnet_folder) or not os.path.exists(yamnet_folder):\n",
    "                print(f\"Warning: Missing folder for {event}/{split}\")\n",
    "                continue\n",
    "            \n",
    "            # Get common files\n",
    "            resnet_files = {f.replace('.npy', '') for f in os.listdir(resnet_folder) if f.endswith('.npy')}\n",
    "            yamnet_files = {f.replace('.npy', '') for f in os.listdir(yamnet_folder) if f.endswith('.npy')}\n",
    "            audio_files = {f.replace('.npy', '') for f in os.listdir(audio_features_folder) if f.endswith('.npy')}\n",
    "            video_files = {f.replace('.npy', '') for f in os.listdir(video_features_folder) if f.endswith('.npy')}\n",
    "            common_files = resnet_files.intersection(yamnet_files).intersection(audio_files).intersection(video_files)\n",
    "            \n",
    "            print(f\"\\n{event}/{split}: {len(common_files)} common files\")\n",
    "            \n",
    "            for file_base in common_files:\n",
    "                try:\n",
    "                    # Load embeddings\n",
    "                    resnet_emb = np.load(os.path.join(resnet_folder, f\"{file_base}.npy\"))\n",
    "                    yamnet_emb = np.load(os.path.join(yamnet_folder, f\"{file_base}.npy\"))\n",
    "                    audio_emb = np.load(os.path.join(audio_features_folder, f\"{file_base}.npy\"))\n",
    "                    video_emb = np.load(os.path.join(video_features_folder, f\"{file_base}.npy\"))\n",
    "                    \n",
    "                    # Fuse embeddings\n",
    "                    fused_emb = fuse_embeddings(resnet_emb, yamnet_emb, audio_emb, video_emb, fusion_method)\n",
    "                    \n",
    "                    # Save fused embedding\n",
    "                    output_path = os.path.join(output_folder, f\"{file_base}.npy\")\n",
    "                    np.save(output_path, fused_emb)\n",
    "                    \n",
    "                    total_fused += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_base} for {event}/{split}: {e}\")\n",
    "            \n",
    "            print(f\"  Saved {len(common_files)} fused embeddings\")\n",
    "    \n",
    "    print(f\"\\nTotal fused embeddings created: {total_fused}\")\n",
    "    return total_fused\n",
    "\n",
    "# Create fused embeddings\n",
    "if __name__ == \"__main__\":\n",
    "    resnet_base_path = \"F:/AIM Lab/Experiment/sliding-window/Resnet-50 embeddings rgb\"\n",
    "    yamnet_base_path = \"F:/AIM Lab/Experiment/sliding-window/Yamnet-embeddings\"\n",
    "    audio_features_path = \"F:/AIM Lab/Experiment/sliding-window/audio_features\"\n",
    "    video_features_path = \"F:/AIM Lab/Experiment/sliding-window/video_features\"\n",
    "\n",
    "    output_base_path = \"F:/AIM Lab/Experiment/sliding-window/Fused_final2\"\n",
    "    \n",
    "    event_classes = [\"Goal\", \"Red_card\", \"Yellow_card\", \"Direct_free-kick\", \"Penalty\",\"Indirect_free-kick\", \"Substitution\", \"Shots_on_target\", \"Corner\",  \"no_event\"]\n",
    "    \n",
    "    # Try different fusion methods\n",
    "    fusion_methods = 'concat'\n",
    "    \n",
    "    method_output_path = f\"{output_base_path}_{fusion_methods}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CREATING FUSED EMBEDDINGS: {fusion_methods.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total = create_fused_embeddings(\n",
    "        resnet_base_path=resnet_base_path,\n",
    "        yamnet_base_path=yamnet_base_path,\n",
    "        audio_features_path=audio_features_path,\n",
    "        video_features_path=video_features_path,\n",
    "        output_base_path=method_output_path,\n",
    "        event_classes=event_classes,\n",
    "        fusion_method=fusion_methods\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {total} fused embeddings using {fusion_methods} method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18817c08",
   "metadata": {},
   "source": [
    "## Charts/Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(results, event_classes, save_path='Trained models/confusion_matrix.png'):\n",
    "    \"\"\"\n",
    "    Create an enhanced confusion matrix visualization\n",
    "    \"\"\"\n",
    "    cm = results['confusion_matrix']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, \n",
    "                annot=True, \n",
    "                fmt='d', \n",
    "                cmap='Blues',\n",
    "                xticklabels=event_classes,\n",
    "                yticklabels=event_classes,\n",
    "                cbar_kws={'label': 'Number of Predictions'})\n",
    "    \n",
    "    plt.title('Confusion Matrix - Prototypical Network\\nRows: True Labels, Columns: Predicted Labels', \n",
    "              fontsize=14, pad=20)\n",
    "    plt.xlabel('Predicted Labels', fontsize=12)\n",
    "    plt.ylabel('True Labels', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Add accuracy text\n",
    "    overall_acc = results['overall_accuracy']\n",
    "    plt.figtext(0.02, 0.02, f'Overall Accuracy: {overall_acc:.3f}', \n",
    "                fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_per_class_metrics(results, event_classes, save_path='Trained models/per_class_metrics.png'):\n",
    "    \"\"\"\n",
    "    Plot per-class precision, recall, F1-score, and accuracy.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "    y_true = results['y_true']\n",
    "    y_pred = results['y_pred']\n",
    "\n",
    "    # existing per-class precision, recall, f1\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred,\n",
    "        labels=list(range(len(event_classes))),\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # new: pull per-class accuracy from results\n",
    "    accuracies = [results['class_accuracies'].get(i, 0.0) for i in range(len(event_classes))]\n",
    "\n",
    "    x = np.arange(len(event_classes))\n",
    "    width = 0.2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    bars1 = ax.bar(x - 1.5*width, precision, width, label='Precision', color='skyblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x - 0.5*width, recall,    width, label='Recall',    color='lightcoral', alpha=0.8)\n",
    "    bars3 = ax.bar(x + 0.5*width, f1,        width, label='F1-Score',  color='lightgreen', alpha=0.8)\n",
    "    bars4 = ax.bar(x + 1.5*width, accuracies,width, label='Accuracy',  color='gold',       alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Event Classes', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Per-Class Performance Metrics', fontsize=14, pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(event_classes, rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    def add_value_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    add_value_labels(bars1)\n",
    "    add_value_labels(bars2)\n",
    "    add_value_labels(bars3)\n",
    "    add_value_labels(bars4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_class_distribution(embeddings_dict, event_classes, save_path='Trained models/class_distribution.png'):\n",
    "    \"\"\"\n",
    "    Visualize class distribution in training data\n",
    "    \"\"\"\n",
    "    train_counts = []\n",
    "    test_counts = []\n",
    "    \n",
    "    for event in event_classes:\n",
    "        train_data = embeddings_dict.get(event, [])\n",
    "        train_counts.append(len(train_data))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar chart\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(event_classes)))\n",
    "    bars = ax1.bar(event_classes, train_counts, color=colors, alpha=0.8)\n",
    "    ax1.set_title('Training Data Distribution by Class', fontsize=14)\n",
    "    ax1.set_xlabel('Event Classes', fontsize=12)\n",
    "    ax1.set_ylabel('Number of Samples', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, train_counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + max(train_counts)*0.01,\n",
    "                f'{count}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(train_counts, labels=event_classes, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "    ax2.set_title('Training Data Class Distribution (%)', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd793e",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "class EpisodicDataLoader:\n",
    "    def __init__(self, embeddings_dict: Dict[str, List[np.ndarray]], n_way: int, k_shot: int, n_query: int):\n",
    "        \"\"\"\n",
    "        Initialize episodic data loader.\n",
    "        \n",
    "        Args:\n",
    "            embeddings_dict: Dictionary of event classes and their embeddings.\n",
    "            n_way: Number of classes per episode.\n",
    "            k_shot: Number of support samples per class.\n",
    "            n_query: Number of query samples per class.\n",
    "        \"\"\"\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        self.class_names = list(embeddings_dict.keys())\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "\n",
    "    def sample_episode(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample one episode with support and query sets.\n",
    "        \n",
    "        Returns:\n",
    "            support_x: Support embeddings (N_support, embedding_dim).\n",
    "            support_y: Support labels (N_support,).\n",
    "            query_x: Query embeddings (N_query, embedding_dim).\n",
    "            query_y: Query labels (N_query,).\n",
    "        \"\"\"\n",
    "        # Randomly sample n_way classes\n",
    "        episode_classes = np.random.choice(self.class_names, size=self.n_way, replace=False)\n",
    "\n",
    "        support_x, support_y, query_x, query_y = [], [], [], []\n",
    "\n",
    "        for class_idx, class_name in enumerate(episode_classes):\n",
    "            class_embeddings = self.embeddings_dict[class_name]\n",
    "            total_needed = self.k_shot + self.n_query\n",
    "\n",
    "            # Sample embeddings for support and query\n",
    "            if len(class_embeddings) < total_needed:\n",
    "                indices = np.random.choice(len(class_embeddings), size=total_needed, replace=True)\n",
    "            else:\n",
    "                indices = np.random.choice(len(class_embeddings), size=total_needed, replace=False)\n",
    "\n",
    "            for i, idx in enumerate(indices):\n",
    "                if i < self.k_shot:\n",
    "                    support_x.append(class_embeddings[idx])\n",
    "                    support_y.append(class_idx)\n",
    "                else:\n",
    "                    query_x.append(class_embeddings[idx])\n",
    "                    query_y.append(class_idx)\n",
    "\n",
    "        # Convert to tensors\n",
    "        support_x = torch.FloatTensor(np.stack(support_x))\n",
    "        support_y = torch.LongTensor(support_y)\n",
    "        query_x = torch.FloatTensor(np.stack(query_x))\n",
    "        query_y = torch.LongTensor(query_y)\n",
    "\n",
    "        return support_x, support_y, query_x, query_y\n",
    "    \n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = 3138, hidden_dim: int = 512, dropout_rate: float = 0.4):\n",
    "        \"\"\"\n",
    "        Initialize Prototypical Network.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of the input embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # a simple trainable MLP\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim)\n",
    "        )\n",
    "        # Learnable temperature parameter for better calibration\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def compute_prototypes(self, support_x: torch.Tensor, support_y: torch.Tensor, n_way: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute class prototypes by averaging support embeddings.\n",
    "        \n",
    "        Args:\n",
    "            support_x: Support embeddings (N_support, embedding_dim).\n",
    "            support_y: Support labels (N_support,).\n",
    "            n_way: Number of classes.\n",
    "        \n",
    "        Returns:\n",
    "            prototypes: Class prototypes (n_way, embedding_dim).\n",
    "        \"\"\"\n",
    "        prototypes = torch.zeros(n_way, self.embedding_dim).to(support_x.device)\n",
    "        for class_idx in range(n_way):\n",
    "            class_mask = (support_y == class_idx)\n",
    "            class_embeddings = support_x[class_mask]\n",
    "            prototypes[class_idx] = class_embeddings.mean(dim=0)\n",
    "        return prototypes\n",
    "\n",
    "    def forward(self, support_x: torch.Tensor, support_y: torch.Tensor, query_x: torch.Tensor, n_way: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Prototypical Network.\n",
    "        \n",
    "        Args:\n",
    "            support_x: Support embeddings (N_support, embedding_dim).\n",
    "            support_y: Support labels (N_support,).\n",
    "            query_x: Query embeddings (N_query, embedding_dim).\n",
    "            n_way: Number of classes.\n",
    "        \n",
    "        Returns:\n",
    "            log_probs: Log probabilities for query samples (N_query, n_way).\n",
    "        \"\"\"\n",
    "        # project both support and query through your feature extractor\n",
    "        support_x = self.feature_extractor(support_x)\n",
    "        query_x   = self.feature_extractor(query_x)\n",
    "        \n",
    "        # Compute prototypes\n",
    "        prototypes = self.compute_prototypes(support_x, support_y, n_way)\n",
    "\n",
    "        # Compute distances between query samples and prototypes\n",
    "        distances = torch.cdist(query_x, prototypes, p=2)  # Euclidean distance\n",
    "\n",
    "        # Apply temperature scaling to logits\n",
    "        logits = -distances / self.temperature\n",
    "\n",
    "        # Convert distances to log probabilities\n",
    "        log_probs = F.log_softmax(-distances, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "def train_prototypical_network(\n",
    "    embeddings_dict: Dict[str, List[np.ndarray]],\n",
    "    n_way: int = 5,\n",
    "    k_shot: int = 5,\n",
    "    n_query: int = 15,\n",
    "    num_episodes: int = 1500,\n",
    "    learning_rate: float = 1e-3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the Prototypical Network using episodic training.\n",
    "    \"\"\"\n",
    "    model = PrototypicalNetwork(embedding_dim=3138)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    data_loader = EpisodicDataLoader(embeddings_dict, n_way, k_shot, n_query)\n",
    "\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        support_x, support_y, query_x, query_y = data_loader.sample_episode()\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(support_x, support_y, query_x, n_way)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(log_probs, query_y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if episode % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                predictions = torch.argmax(log_probs, dim=1)\n",
    "                accuracy = (predictions == query_y).float().mean().item()\n",
    "                print(f\"Episode {episode}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_prototypical_network(\n",
    "    model: PrototypicalNetwork,\n",
    "    test_embeddings_dict: Dict[str, List[np.ndarray]],\n",
    "    n_way: int = 5,\n",
    "    k_shot: int = 5,\n",
    "    n_query: int = 10,\n",
    "    num_episodes: int = 100\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the Prototypical Network on test episodes and compute full metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader = EpisodicDataLoader(test_embeddings_dict, n_way, k_shot, n_query)\n",
    "\n",
    "    accuracies, y_true, y_pred = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_episodes):\n",
    "            support_x, support_y, query_x, query_y = data_loader.sample_episode()\n",
    "            log_probs = model(support_x, support_y, query_x, n_way)\n",
    "            preds = torch.argmax(log_probs, dim=1)\n",
    "            accuracies.append((preds == query_y).float().mean().item())\n",
    "\n",
    "            y_true.extend(query_y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    overall_accuracy = np.mean(accuracies)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(n_way)))\n",
    "    class_report = classification_report(\n",
    "        y_true, y_pred, labels=list(range(n_way)), zero_division=0\n",
    "    )\n",
    "    precision = precision_score(\n",
    "        y_true, y_pred, labels=list(range(n_way)), average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    recall = recall_score(\n",
    "        y_true, y_pred, labels=list(range(n_way)), average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    f1 = f1_score(\n",
    "        y_true, y_pred, labels=list(range(n_way)), average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # per-class accuracy\n",
    "    class_accuracies = {}\n",
    "    for idx in range(n_way):\n",
    "        mask = np.array(y_true) == idx\n",
    "        correct = np.sum((np.array(y_true) == np.array(y_pred)) & mask)\n",
    "        total = np.sum(mask)\n",
    "        class_accuracies[idx] = correct / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": class_report,\n",
    "        \"class_accuracies\": class_accuracies,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "        \n",
    "def load_embeddings_for_meta_learning(base_path, event_classes, split):\n",
    "    \"\"\"\n",
    "    Load embeddings into a dictionary for meta-learning.\n",
    "    Args:\n",
    "        base_path (str): Path to the embeddings folder.\n",
    "        event_classes (list): List of event class names.\n",
    "        split (str): \"Train\" or \"Test\".\n",
    "    Returns:\n",
    "        dict: Dictionary with class names as keys and list of embeddings as values.\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    for event in event_classes:\n",
    "        folder = os.path.join(base_path, event, split)\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"Warning: Folder not found for {event}/{split}\")\n",
    "            continue\n",
    "        embeddings_dict[event] = [np.load(os.path.join(folder, f)) for f in os.listdir(folder) if f.endswith('.npy')]\n",
    "    return embeddings_dict\n",
    "\n",
    "def generate_detailed_evaluation_report(results, event_classes, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Print a detailed evaluation report.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 3: Additional Analysis...\")\n",
    "    print(f\"Total evaluation samples: {len(results['y_true'])}\")\n",
    "    print(f\"Number of classes: {len(event_classes)}\")\n",
    "\n",
    "    cm = results[\"confusion_matrix\"]\n",
    "    print(\"\\nMost confused class pairs:\")\n",
    "    for i, t in enumerate(event_classes):\n",
    "        for j, p in enumerate(event_classes):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                print(f\"  {t} → {p}: {cm[i, j]} times\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Overall Accuracy: {results['overall_accuracy']:.4f}\")\n",
    "    print(f\"Weighted Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Weighted Recall: {results['recall']:.4f}\")\n",
    "    print(f\"Weighted F₁ Score: {results['f1_score']:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for idx, acc in results[\"class_accuracies\"].items():\n",
    "        print(f\"  {event_classes[idx]}: {acc:.4f}\")\n",
    "    \n",
    "    plot_confusion_matrix(results, event_classes)\n",
    "\n",
    "    plot_per_class_metrics(results, event_classes)\n",
    "\n",
    "    plot_class_distribution(embeddings_dict, event_classes)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"Rows: True labels, Columns: Predicted labels\")\n",
    "    print(f\"Classes: {event_classes}\")\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(results[\"classification_report\"])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MODIFIED MULTICLASS FEW-SHOT LEARNING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def save_few_shot_model(model, embeddings_base_path, event_classes, save_dir=\"Trained models\"):\n",
    "    \"\"\"\n",
    "    Save the trained few-shot model and related components\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PrototypicalNetwork\n",
    "        embeddings_base_path: Path to embeddings used for training\n",
    "        event_classes: List of event class names\n",
    "        save_dir: Directory to save the model\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    model_path = os.path.join(save_dir, \"prototypical_network.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model to: {model_path}\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    config = {\n",
    "        'embedding_dim': model.embedding_dim,\n",
    "        'hidden_dim': 512,  # From your model definition\n",
    "        'dropout_rate': 0.3,  # From your model definition\n",
    "        'event_classes': event_classes,\n",
    "        'embeddings_base_path': embeddings_base_path\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(save_dir, \"model_config.json\")\n",
    "    import json\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"Saved config to: {config_path}\")\n",
    "    \n",
    "    return model_path, config_path\n",
    "# Step 3: Generate ResNet-50 Embeddings\n",
    "def main():\n",
    "\n",
    "    # Step 4: Few-Shot Learning\n",
    "    embeddings_base_path = \"F:\\AIM Lab\\Experiment\\sliding-window\\Fused_final2_concat\"\n",
    "    event_classes = [\"Goal\", \"Penalty\", \"Direct_free-kick\", \"Red_card\", \"Substitution\", \"no_event\"]\n",
    "    # use all classes per episode\n",
    "    n_way, k_shot, n_query = len(event_classes), 5, 15\n",
    "    # 4.1: Train Prototypical Network\n",
    "    print(\"Training Prototypical Network...\")\n",
    "    embeddings_dict = load_embeddings_for_meta_learning(embeddings_base_path, event_classes, split=\"Train\")  # Implement this function to load embeddings into a dictionary\n",
    "    model = train_prototypical_network(embeddings_dict, n_way=n_way, k_shot=k_shot, n_query=n_query, num_episodes=1500)\n",
    "\n",
    "    # 4.2: Evaluate Prototypical Network\n",
    "    print(\"Evaluating Prototypical Network...\")\n",
    "    test_embeddings_dict = load_embeddings_for_meta_learning(\n",
    "        embeddings_base_path, event_classes, split=\"Test\"\n",
    "    )\n",
    "    results = evaluate_prototypical_network(\n",
    "        model, test_embeddings_dict, n_way=n_way, k_shot=k_shot, n_query=2, num_episodes=100\n",
    "    )\n",
    "\n",
    "    generate_detailed_evaluation_report(results, event_classes, embeddings_dict)\n",
    "\n",
    "    # NEW: Save the trained model\n",
    "    model_path, config_path = save_few_shot_model(\n",
    "        model, embeddings_base_path, event_classes\n",
    "    )\n",
    "    print(f\"\\nModel saved successfully!\")\n",
    "    print(f\"Model path: {model_path}\")\n",
    "    print(f\"Config path: {config_path}\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
