{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57398135",
   "metadata": {},
   "source": [
    "## Load JSON segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "with open(\"F:\\\\AIM Lab\\\\Project\\\\whisper_v2_en\\\\england_epl\\\\2014-2015\\\\2015-05-17 - 18-00 Manchester United 1 - 1 Arsenal\\\\2_asr.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "segments = [\n",
    "    (float(v[0]), float(v[1]), v[2])\n",
    "    for k,v in data[\"segments\"].items()\n",
    "]\n",
    "\n",
    "resnet_embedding_path = \"F:\\\\AIM Lab\\\\Project\\\\SoccerNet\\\\england_epl\\\\2014-2015\\\\2015-05-17 - 18-00 Manchester United 1 - 1 Arsenal\\\\2_ResNET_TF2_PCA512.npy\"  # Update with the correct path\n",
    "resnet_embeddings = np.load(resnet_embedding_path)\n",
    "rows = resnet_embeddings.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c148e87",
   "metadata": {},
   "source": [
    "## Compute BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")  # or all-mpet-basne-v2 for higher quality\n",
    "\n",
    "# get embeddings for all segments\n",
    "embeddings = []\n",
    "for (start, end, text) in segments:\n",
    "    emb = model.encode(text, convert_to_tensor=True)\n",
    "    embeddings.append((start, end, emb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ce8cb",
   "metadata": {},
   "source": [
    "## Build 0.5 s timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83613ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dt = 0.5\n",
    "t_min = 0\n",
    "t_max = (rows - 1) * dt\n",
    "\n",
    "time_grid = np.arange(t_min, t_max+dt, dt)\n",
    "N = len(time_grid)\n",
    "D = embeddings[0][2].shape[0]\n",
    "\n",
    "timeline_embeddings = torch.zeros((N, D))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22abf67b",
   "metadata": {},
   "source": [
    "## Forward-fill with last embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d082bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_emb = torch.zeros(D)\n",
    "last_time = -1e9  # very far in past\n",
    "\n",
    "for i, t in enumerate(time_grid):\n",
    "    # find active segment\n",
    "    hit = next((emb for (start, end, emb) in embeddings if start <= t < end), None)\n",
    "    if hit is not None:\n",
    "        timeline_embeddings[i] = hit\n",
    "        last_emb = hit\n",
    "        last_time = t\n",
    "    else:\n",
    "        # silence: forward-fill if gap ≤5 s, else zeros\n",
    "        if t - last_time <= 5.0:\n",
    "            timeline_embeddings[i] = last_emb\n",
    "        else:\n",
    "            timeline_embeddings[i] = torch.zeros(D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeline_embeddings.shape)\n",
    "print(timeline_embeddings[:5])  # first few time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fee5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_embedding_path = \"F:\\\\AIM Lab\\\\Project\\\\SoccerNet\\\\england_epl\\\\2014-2015\\\\2015-05-17 - 18-00 Manchester United 1 - 1 Arsenal\\\\2_ResNET_TF2_PCA512.npy\"  # Update with the correct path\n",
    "resnet_embeddings = np.load(resnet_embedding_path)\n",
    "video_embeddings = torch.tensor(resnet_embeddings, dtype=torch.float32)\n",
    "T = video_embeddings.shape[0]  \n",
    "# View the first 4 rows\n",
    "print(resnet_embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac849e90",
   "metadata": {},
   "source": [
    "### Time since last speech scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db64e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_last_speech = torch.zeros((T, 1))\n",
    "last_speech_time = -1e9  # very far in past\n",
    "\n",
    "# rebuild segments only start times\n",
    "segment_starts = torch.tensor([s[0] for s in segments])\n",
    "\n",
    "for i in range(T):\n",
    "    t = i * dt\n",
    "    # check if any speech segment started now\n",
    "    if (segment_starts == t).any():\n",
    "        last_speech_time = t\n",
    "    time_since_last_speech[i] = t - last_speech_time\n",
    "\n",
    "# normalize: clip & scale to roughly 0..1\n",
    "time_since_last_speech = torch.clamp(time_since_last_speech / 10.0, 0, 1)  # e.g. 10 sec -> 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7c017",
   "metadata": {},
   "source": [
    "### positional encodings (relative time in match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21702ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def positional_encoding(T, dim_pe, max_time):\n",
    "    pe = torch.zeros((T, dim_pe))\n",
    "    position = torch.arange(0, T).unsqueeze(1).float()  # [T,1]\n",
    "    position = position / T * max_time  # scale to match actual time\n",
    "\n",
    "    div_term = torch.exp(torch.arange(0, dim_pe, 2) * (-math.log(10000.0) / dim_pe))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "dim_pe = 16\n",
    "max_time = T * dt  # e.g. total seconds in video\n",
    "pos_encodings = positional_encoding(T, dim_pe, max_time)  # [T, 16]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672eab7",
   "metadata": {},
   "source": [
    "### concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossModalTransformerFusion(nn.Module):\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=2, dropout=0.1):\n",
    "        super(CrossModalTransformerFusion, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Multi-head attention for cross-modal fusion\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Transformer encoder layers for further processing\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=2048,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, video_emb, text_emb):\n",
    "        # video_emb: [T, d_model]\n",
    "        # text_emb: [T, d_model]\n",
    "        \n",
    "        # Add batch dimension if needed\n",
    "        if video_emb.dim() == 2:\n",
    "            video_emb = video_emb.unsqueeze(0)  # [1, T, d_model]\n",
    "        if text_emb.dim() == 2:\n",
    "            text_emb = text_emb.unsqueeze(0)    # [1, T, d_model]\n",
    "        \n",
    "        # Cross-modal attention: video attends to text\n",
    "        video_attended, _ = self.cross_attention(\n",
    "            query=video_emb,\n",
    "            key=text_emb,\n",
    "            value=text_emb\n",
    "        )\n",
    "        \n",
    "        # Cross-modal attention: text attends to video\n",
    "        text_attended, _ = self.cross_attention(\n",
    "            query=text_emb,\n",
    "            key=video_emb,\n",
    "            value=video_emb\n",
    "        )\n",
    "        \n",
    "        # Residual connections\n",
    "        video_fused = self.layer_norm(video_emb + video_attended)\n",
    "        text_fused = self.layer_norm(text_emb + text_attended)\n",
    "        \n",
    "        # Concatenate the fused embeddings\n",
    "        multimodal_emb = torch.cat([video_fused, text_fused], dim=-1)  # [1, T, 2*d_model]\n",
    "        \n",
    "        # Apply transformer encoder for final fusion\n",
    "        fused_output = self.transformer_encoder(multimodal_emb)\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        return fused_output.squeeze(0)  # [T, 2*d_model]\n",
    "\n",
    "# Initialize the fusion module\n",
    "fusion_module = CrossModalTransformerFusion(d_model=512, nhead=8, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Ensure timeline_embeddings matches video_embeddings length\n",
    "T = video_embeddings.shape[0]\n",
    "if timeline_embeddings.shape[0] != T:\n",
    "    if timeline_embeddings.shape[0] > T:\n",
    "        timeline_embeddings = timeline_embeddings[:T]\n",
    "    else:\n",
    "        # Pad with zeros if timeline is shorter\n",
    "        padding = torch.zeros(T - timeline_embeddings.shape[0], timeline_embeddings.shape[1])\n",
    "        timeline_embeddings = torch.cat([timeline_embeddings, padding], dim=0)\n",
    "\n",
    "D_video = video_embeddings.shape[1]    # e.g. 512\n",
    "D_text  = timeline_embeddings.shape[1] # e.g. 384\n",
    "D_proj  = 512                          # common dimension\n",
    "\n",
    "# Normalization layers\n",
    "video_norm = nn.LayerNorm(D_video)\n",
    "text_norm  = nn.LayerNorm(D_text)\n",
    "\n",
    "# Projection layers\n",
    "video_proj = nn.Linear(D_video, D_proj)\n",
    "text_proj  = nn.Linear(D_text, D_proj)\n",
    "\n",
    "# Normalize\n",
    "video_normalized = video_norm(video_embeddings)\n",
    "text_normalized  = text_norm(timeline_embeddings)\n",
    "\n",
    "# Project to same dim\n",
    "video_projected = video_proj(video_normalized)  # [T, 512]\n",
    "text_projected  = text_proj(text_normalized)    # [T, 512]\n",
    "\n",
    "multimodal_embeddings = fusion_module(video_projected, text_projected)\n",
    "\n",
    "# Concatenate with additional features\n",
    "full_embeddings = torch.cat([\n",
    "    multimodal_embeddings,      # [T, 1024]\n",
    "    time_since_last_speech,     # [T, 1]\n",
    "    pos_encodings               # [T, 16]\n",
    "], dim=1)                       # ➔ [T, 1041]\n",
    "\n",
    "# Final normalization\n",
    "norm = nn.LayerNorm(full_embeddings.shape[1])\n",
    "full_embeddings = norm(full_embeddings)\n",
    "\n",
    "print(full_embeddings.shape)\n",
    "print(full_embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# 1. Load one ASR JSON\n",
    "json_path = \"F:\\\\AIM Lab\\\\Project\\\\whisper_v2_en\\\\england_epl\\\\2014-2015\\\\2015-05-17 - 18-00 Manchester United 1 - 1 Arsenal\\\\2_asr.json\"\n",
    "with open(json_path, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "segments = [(float(v[0]), float(v[1]), v[2]) for v in data[\"segments\"].values()]\n",
    "\n",
    "# 2. Build timeline embeddings in NumPy (use simple one-hot/silence mask to verify)\n",
    "dt = 0.5\n",
    "T = 1000  # or compute from your corresponding ResNet length\n",
    "time_grid = np.arange(0, T*dt, dt)\n",
    "\n",
    "mask = np.zeros(T)\n",
    "for (start, end, _) in segments:\n",
    "    idx_start = int(start / dt)\n",
    "    idx_end   = int(end   / dt)\n",
    "    mask[idx_start:idx_end] = 1\n",
    "\n",
    "# 3. Plot the mask to verify alignment\n",
    "times = time_grid\n",
    "plt.figure()\n",
    "plt.plot(times, mask)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Speech Present\")\n",
    "plt.title(\"Speech Segment Alignment (1=present, 0=silence)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
